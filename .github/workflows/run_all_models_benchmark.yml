name: Run All Models Benchmarks (Manual)

on:
  # Manual trigger only
  workflow_dispatch:
    inputs:
      models:
        description: 'Models to run (comma-separated or "all")'
        required: false
        default: 'all'

jobs:
  # Run each model in parallel for faster execution
  benchmark-medgemma:
    if: contains(github.event.inputs.models, 'all') || contains(github.event.inputs.models, 'medgemma')
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
      - uses: actions/checkout@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e . -v
          pip install -r requirements-benchmarks.txt
      
      - name: Wake up and warmup HuggingFace endpoint
        env:
          HF_API_TOKEN: ${{ secrets.HF_API_TOKEN }}
          HF_ENDPOINT_BASE: ${{ secrets.HF_ENDPOINT_BASE }}
          HF_MODEL_ID: google/medgemma-4b-it
        run: |
          echo "üîç Checking HuggingFace Inference Endpoint status..."
          
          # Make a real inference request to wake up the endpoint
          echo "üöÄ Sending warmup request to wake up endpoint..."
          curl -X POST "${HF_ENDPOINT_BASE}/v1/chat/completions" \
            -H "Authorization: Bearer ${HF_API_TOKEN}" \
            -H "Content-Type: application/json" \
            -d '{
              "model": "google/medgemma-4b-it",
              "messages": [{"role": "user", "content": "Test"}],
              "max_tokens": 10
            }' || echo "Warmup request sent (may get 503 initially)"
          
          echo "‚è≥ Waiting 5 minutes for endpoint to initialize..."
          echo "This allows the model to load into memory and become ready."
          sleep 300
          
          # Verify endpoint is ready
          echo "üîç Verifying endpoint is ready..."
          for i in {1..6}; do
            STATUS=$(curl -s -o /dev/null -w "%{http_code}" -X POST "${HF_ENDPOINT_BASE}/v1/chat/completions" \
              -H "Authorization: Bearer ${HF_API_TOKEN}" \
              -H "Content-Type: application/json" \
              -d '{"model": "google/medgemma-4b-it", "messages": [{"role": "user", "content": "Ready?"}], "max_tokens": 5}')
            
            if [ "$STATUS" = "200" ]; then
              echo "‚úÖ Endpoint is ready and responding!"
              break
            else
              echo "‚è≥ Attempt $i/6: Status $STATUS, waiting 30 more seconds..."
              sleep 30
            fi
          done
      
      - name: Run MedGemma benchmarks
        env:
          HF_API_TOKEN: ${{ secrets.HF_API_TOKEN }}
          HF_ENDPOINT_BASE: ${{ secrets.HF_ENDPOINT_BASE }}
          HF_MODEL_ID: google/medgemma-4b-it
        run: |
          python3 scripts/generate_patient_benchmarks.py --model medgemma --workers 1
      
      - name: Push results to Supabase
        if: always()
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          if [ -f "benchmarks/results/patient_benchmark_medgemma.json" ]; then
            echo "üì§ Pushing medgemma results to Supabase..."
            python3 scripts/push_patient_benchmarks.py \
              --input benchmarks/results/patient_benchmark_medgemma.json \
              --environment github-actions \
              --commit-sha ${{ github.sha }} \
              --branch-name ${{ github.ref_name }} \
              --triggered-by ${{ github.actor }} || echo "‚ö†Ô∏è Failed to push results (validation may have rejected them)"
          else
            echo "‚ö†Ô∏è No results file found to push"
          fi
      
      - name: Upload MedGemma results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-medgemma
          path: benchmarks/results/patient_benchmark_medgemma.json
          retention-days: 90

  benchmark-medgemma-ensemble:
    if: contains(github.event.inputs.models, 'all') || contains(github.event.inputs.models, 'medgemma-ensemble')
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
      - uses: actions/checkout@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e . -v
          pip install -r requirements-benchmarks.txt
      
      - name: Wake up and warmup HuggingFace endpoint
        env:
          HF_API_TOKEN: ${{ secrets.HF_API_TOKEN }}
          HF_ENDPOINT_BASE: ${{ secrets.HF_ENDPOINT_BASE }}
          HF_MODEL_ID: google/medgemma-4b-it
        run: |
          echo "üîç Checking HuggingFace Inference Endpoint status..."
          
          # Make a real inference request to wake up the endpoint
          echo "üöÄ Sending warmup request to wake up endpoint..."
          curl -X POST "${HF_ENDPOINT_BASE}/v1/chat/completions" \
            -H "Authorization: Bearer ${HF_API_TOKEN}" \
            -H "Content-Type: application/json" \
            -d '{
              "model": "google/medgemma-4b-it",
              "messages": [{"role": "user", "content": "Test"}],
              "max_tokens": 10
            }' || echo "Warmup request sent (may get 503 initially)"
          
          echo "‚è≥ Waiting 5 minutes for endpoint to initialize..."
          echo "This allows the model to load into memory and become ready."
          sleep 300
          
          # Verify endpoint is ready
          echo "üîç Verifying endpoint is ready..."
          for i in {1..6}; do
            STATUS=$(curl -s -o /dev/null -w "%{http_code}" -X POST "${HF_ENDPOINT_BASE}/v1/chat/completions" \
              -H "Authorization: Bearer ${HF_API_TOKEN}" \
              -H "Content-Type: application/json" \
              -d '{"model": "google/medgemma-4b-it", "messages": [{"role": "user", "content": "Ready?"}], "max_tokens": 5}')
            
            if [ "$STATUS" = "200" ]; then
              echo "‚úÖ Endpoint is ready and responding!"
              break
            else
              echo "‚è≥ Attempt $i/6: Status $STATUS, waiting 30 more seconds..."
              sleep 30
            fi
          done
      
      - name: Run MedGemma Ensemble benchmarks
        env:
          HF_API_TOKEN: ${{ secrets.HF_API_TOKEN }}
          HF_ENDPOINT_BASE: ${{ secrets.HF_ENDPOINT_BASE }}
          HF_MODEL_ID: google/medgemma-4b-it
        run: |
          python3 scripts/generate_patient_benchmarks.py --model medgemma-ensemble --workers 1
      
      - name: Push results to Supabase
        if: always()
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          if [ -f "benchmarks/results/patient_benchmark_medgemma-ensemble.json" ]; then
            echo "üì§ Pushing medgemma-ensemble results to Supabase..."
            python3 scripts/push_patient_benchmarks.py \
              --input benchmarks/results/patient_benchmark_medgemma-ensemble.json \
              --environment github-actions \
              --commit-sha ${{ github.sha }} \
              --branch-name ${{ github.ref_name }} \
              --triggered-by ${{ github.actor }} || echo "‚ö†Ô∏è Failed to push results (validation may have rejected them)"
          else
            echo "‚ö†Ô∏è No results file found to push"
          fi
      
      - name: Upload MedGemma Ensemble results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-medgemma-ensemble
          path: benchmarks/results/patient_benchmark_medgemma-ensemble.json
          retention-days: 90

  benchmark-openai:
    if: contains(github.event.inputs.models, 'all') || contains(github.event.inputs.models, 'openai')
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
      - uses: actions/checkout@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e . -v
          pip install -r requirements-benchmarks.txt
      
      - name: Run OpenAI benchmarks
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python3 scripts/generate_patient_benchmarks.py --model openai --workers 1
      
      - name: Upload OpenAI results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-openai
          path: benchmarks/results/patient_benchmark_openai.json
          retention-days: 90

  benchmark-baseline:
    if: contains(github.event.inputs.models, 'all') || contains(github.event.inputs.models, 'baseline')
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
      - uses: actions/checkout@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e . -v
          pip install -r requirements-benchmarks.txt
      
      - name: Wake up HuggingFace endpoint (if using MedGemma models)
        if: contains(github.event.inputs.models, 'all') || contains(github.event.inputs.models, 'medgemma')
        env:
          HF_API_TOKEN: ${{ secrets.HF_API_TOKEN }}
          HF_ENDPOINT_BASE: ${{ secrets.HF_ENDPOINT_BASE }}
          HF_MODEL_ID: google/medgemma-4b-it
        run: |
          echo "üîç Checking HuggingFace Inference Endpoint status..."
          
          # Make a real inference request to wake up the endpoint
          echo "üöÄ Sending warmup request to wake up endpoint..."
          curl -X POST "${HF_ENDPOINT_BASE}/v1/chat/completions" \
            -H "Authorization: Bearer ${HF_API_TOKEN}" \
            -H "Content-Type: application/json" \
            -d '{
              "model": "google/medgemma-4b-it",
              "messages": [{"role": "user", "content": "Test"}],
              "max_tokens": 10
            }' || echo "Warmup request sent (may get 503 initially)"
          
          echo "‚è≥ Waiting 5 minutes for endpoint to initialize..."
          echo "This allows the model to load into memory and become ready."
          sleep 300
          
          # Verify endpoint is ready
          echo "üîç Verifying endpoint is ready..."
          for i in {1..6}; do
            STATUS=$(curl -s -o /dev/null -w "%{http_code}" -X POST "${HF_ENDPOINT_BASE}/v1/chat/completions" \
              -H "Authorization: Bearer ${HF_API_TOKEN}" \
              -H "Content-Type: application/json" \
              -d '{"model": "google/medgemma-4b-it", "messages": [{"role": "user", "content": "Ready?"}], "max_tokens": 5}')
            
            if [ "$STATUS" = "200" ]; then
              echo "‚úÖ Endpoint is ready and responding!"
              break
            else
              echo "‚è≥ Attempt $i/6: Status $STATUS, waiting 30 more seconds..."
              sleep 30
            fi
          done
      
      - name: Run benchmarks
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          HF_API_TOKEN: ${{ secrets.HF_API_TOKEN }}
          HF_ENDPOINT_BASE: ${{ secrets.HF_ENDPOINT_BASE }}
          HF_MODEL_ID: google/medgemma-4b-it
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
        run: |
          MODELS="${{ github.event.inputs.models }}"
          if [ -z "$MODELS" ]; then
            MODELS="all"
          fi
          
          echo "Running benchmarks for models: $MODELS"
          python3 scripts/run_benchmarks_v2.py --model "$MODELS" --workers 1
      
      - name: Convert results to monitoring format
        if: always()
        run: |
          mkdir -p benchmark-artifacts
          for model in baseline openai medgemma medgemma-ensemble; do
            if [ -f "benchmarks/results/patient_benchmark_${model}.json" ]; then
              python3 scripts/convert_benchmark_to_monitoring.py \
                --input "benchmarks/results/patient_benchmark_${model}.json" \
                --output "benchmark-artifacts/${model}_benchmark_results.json" \
                --model "${model}"
            fi
          done
      
      - name: Calculate ROI
        if: always()
        run: |
          python3 scripts/calculate_roi_metrics.py \
            --results-dir benchmark-artifacts \
            --output benchmark-artifacts/roi_summary.json \
            --report benchmark-artifacts/roi_report.md
      
      - name: Push results to Supabase
        if: always()
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          for model in baseline openai medgemma medgemma-ensem                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ble; do
            if [ -f "benchmark-artifacts/${model}_benchmark_results.json" ]; then
              echo "üì§ Pushing ${model} results to Supabase..."
              python3 scripts/push_to_supabase.py \
                --input "benchmark-artifacts/${model}_benchmark_results.json" \
                --environment github-actions-manual \
                --commit-sha ${{ github.sha }} \
                --branch-name ${{ github.ref_name }} \
                --run-id ${{ github.run_id }} \
                --triggered-by ${{ github.actor }} \
                --verify || echo "‚ö†Ô∏è Failed to push ${model} to Supabase"
            fi
          done
      
      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-all-models
          path: |
            benchmark-artifacts/*.json
            benchmark-artifacts/*.md
          retention-days: 90
