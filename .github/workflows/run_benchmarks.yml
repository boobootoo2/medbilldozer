name: Run Benchmarks

on:
  # Run on schedule (daily at 2am UTC)
  schedule:
    - cron: '0 2 * * *'
  
  # Run manually
  workflow_dispatch:
  
  # Run on pushes to develop
  push:
    branches:
      - develop
    paths:
      - 'src/medbilldozer/providers/**'
      - 'src/medbilldozer/extractors/**'
      - 'scripts/generate_benchmarks.py'
      - 'benchmarks/inputs/**'
      - '.github/workflows/run_benchmarks.yml'

jobs:
  benchmark:
    runs-on: ubuntu-latest
    permissions:
      contents: write  # Allow workflow to push commits
    
    steps:
      - uses: actions/checkout@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}  # Use built-in token for auth
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install main dependencies first (required for medbilldozer package)
          pip install -r requirements.txt
          # Install package in editable mode (provides medbilldozer imports)
          pip install -e . -v
          # Install benchmark-specific requirements (may override some versions)
          pip install -r requirements-benchmarks.txt
          # Verify installation
          python -c "import sys; print('Python path:', sys.path); import medbilldozer; print(f'medbilldozer {medbilldozer.__version__} imported successfully from {medbilldozer.__file__}')"
      
      - name: Run benchmarks
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          HF_API_TOKEN: ${{ secrets.HF_API_TOKEN }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
        run: |
          # Run each provider separately to handle failures gracefully
          # Skip MedGemma for now - HF Inference API is unreliable
          python3 scripts/generate_benchmarks.py --model baseline || echo "‚ö†Ô∏è Baseline failed"
          python3 scripts/generate_benchmarks.py --model openai || echo "‚ö†Ô∏è OpenAI failed"
          python3 scripts/generate_benchmarks.py --model gemini || echo "‚ö†Ô∏è Gemini failed"
          python3 scripts/generate_benchmarks.py --model medgemma || echo "‚ö†Ô∏è MedGemma failed"
          python3 scripts/generate_patient_benchmarks.py --model gemma3 || echo "‚ö†Ô∏è Gemma3 patient benchmark failed"
      
      - name: Convert results to monitoring format
        if: always()
        run: |
          mkdir -p benchmark-artifacts
          # Convert each model's results
          for model in openai gemini baseline; do
            if [ -f "benchmarks/results/aggregated_metrics_${model}.json" ]; then
              python3 scripts/convert_benchmark_to_monitoring.py \
                --input "benchmarks/results/aggregated_metrics_${model}.json" \
                --output "benchmark-artifacts/${model}_benchmark_results.json" \
                --model "${model}"
            fi
          done
      
      - name: Push results to Supabase
        if: always()
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          # Push each model's results to Supabase
          for model in openai gemini baseline; do
            if [ -f "benchmark-artifacts/${model}_benchmark_results.json" ]; then
              echo "üì§ Pushing ${model} results to Supabase..."
              python3 scripts/push_to_supabase.py \
                --input "benchmark-artifacts/${model}_benchmark_results.json" \
                --environment github-actions \
                --commit-sha ${{ github.sha }} \
                --branch-name ${{ github.ref_name }} \
                --run-id ${{ github.run_id }} \
                --triggered-by ${{ github.actor }} \
                --verify || echo "‚ö†Ô∏è Failed to push ${model} to Supabase"
            fi
          done
      
      - name: Upload benchmark results as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark-artifacts/*.json
          retention-days: 90
      
      - name: Comment on PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const metrics = JSON.parse(fs.readFileSync('benchmarks/results/aggregated_metrics.json'));
            
            let comment = '## üìä Benchmark Results\n\n';
            comment += '| Model | Precision | Recall | F1 Score | Latency |\n';
            comment += '|-------|-----------|--------|----------|----------|\n';
            
            for (const result of metrics.individual_results || []) {
              const model = result.model_name || 'Unknown';
              const precision = (metrics.issue_precision * 100).toFixed(1);
              const recall = (metrics.issue_recall * 100).toFixed(1);
              const f1 = (metrics.issue_f1_score * 100).toFixed(1);
              const latency = (metrics.avg_pipeline_latency_ms / 1000).toFixed(2);
              
              comment += `| ${model} | ${precision}% | ${recall}% | ${f1}% | ${latency}s |\n`;
            }
            
            comment += '\n[View full results ‚Üí](https://github.com/boobootoo2/medbilldozer/tree/develop/benchmarks/results)\n';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
