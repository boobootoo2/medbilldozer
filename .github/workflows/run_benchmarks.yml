name: Run Benchmarks

on:
  # Run on schedule (daily at 2am UTC)
  schedule:
    - cron: '0 2 * * *'
  
  # Run manually
  workflow_dispatch:
  
  # Run on pushes to develop
  push:
    branches:
      - develop
    paths:
      - '_modules/providers/**'
      - 'scripts/generate_benchmarks.py'
      - 'benchmarks/inputs/**'
      - '.github/workflows/run_benchmarks.yml'

jobs:
  benchmark:
    runs-on: ubuntu-latest
    permissions:
      contents: write  # Allow workflow to push commits
    
    steps:
      - uses: actions/checkout@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}  # Use built-in token for auth
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Use minimal requirements for benchmarks only
          pip install -r requirements-benchmarks.txt
      
      - name: Run benchmarks
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          HF_API_TOKEN: ${{ secrets.HF_API_TOKEN }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
        run: |
          # Run each provider separately to handle failures gracefully
          # Skip MedGemma for now - HF Inference API is unreliable
          python3 scripts/generate_benchmarks.py --model baseline || echo "‚ö†Ô∏è Baseline failed"
          python3 scripts/generate_benchmarks.py --model openai || echo "‚ö†Ô∏è OpenAI failed"
          python3 scripts/generate_benchmarks.py --model gemini || echo "‚ö†Ô∏è Gemini failed"
          # python3 scripts/generate_benchmarks.py --model medgemma || echo "‚ö†Ô∏è MedGemma failed"
      
      - name: Convert results to monitoring format
        if: always()
        run: |
          mkdir -p benchmark-artifacts
          # Convert each model's results
          for model in openai gemini baseline; do
            if [ -f "benchmarks/results/aggregated_metrics_${model}.json" ]; then
              python3 scripts/convert_benchmark_to_monitoring.py \
                --input "benchmarks/results/aggregated_metrics_${model}.json" \
                --output "benchmark-artifacts/${model}_benchmark_results.json" \
                --model "${model}"
            fi
          done
      
      - name: Upload benchmark results as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark-artifacts/*.json
          retention-days: 90
      
      - name: Commit benchmark results
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add benchmarks/results/
          git diff --quiet && git diff --staged --quiet || (git commit -m "üìä Update benchmark results" && git push)
      
      - name: Comment on PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const metrics = JSON.parse(fs.readFileSync('benchmarks/results/aggregated_metrics.json'));
            
            let comment = '## üìä Benchmark Results\n\n';
            comment += '| Model | Precision | Recall | F1 Score | Latency |\n';
            comment += '|-------|-----------|--------|----------|----------|\n';
            
            for (const result of metrics.individual_results || []) {
              const model = result.model_name || 'Unknown';
              const precision = (metrics.issue_precision * 100).toFixed(1);
              const recall = (metrics.issue_recall * 100).toFixed(1);
              const f1 = (metrics.issue_f1_score * 100).toFixed(1);
              const latency = (metrics.avg_pipeline_latency_ms / 1000).toFixed(2);
              
              comment += `| ${model} | ${precision}% | ${recall}% | ${f1}% | ${latency}s |\n`;
            }
            
            comment += '\n[View full results ‚Üí](https://github.com/boobootoo2/medbilldozer/tree/develop/benchmarks/results)\n';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
