#!/usr/bin/env python3
"""
Phase 2: LoRA Fine-Tuning for MedGemma

Fine-tunes google/medgemma-2b using LoRA (Low-Rank Adaptation) on clinical
validation scenarios with vision features from BioMedCLIP.

Expected Improvement: 79% ‚Üí 90% accuracy

Architecture:
- Base Model: google/medgemma-2b (decoder-only, 2B params)
- Vision Features: BioMedCLIP embeddings (512-dim) ‚Üí projection layer
- LoRA: rank=16, alpha=32, dropout=0.1
- Target Modules: q_proj, v_proj, o_proj

Training:
- Epochs: 3-5
- Batch Size: 4 (gradient accumulation)
- Learning Rate: 2e-4
- Optimizer: AdamW with warmup
"""

import json
import sys
import torch
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import argparse

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# Check dependencies
try:
    from transformers import (
        AutoTokenizer, 
        AutoModelForCausalLM,
        TrainingArguments,
        Trainer,
        DataCollatorForLanguageModeling
    )
    from peft import (
        LoraConfig,
        get_peft_model,
        TaskType,
        prepare_model_for_kbit_training
    )
    from datasets import Dataset
    from PIL import Image
    import numpy as np
except ImportError as e:
    print(f"‚ùå Missing dependency: {e}")
    print("\nInstall with:")
    print("  pip install transformers peft datasets pillow torch")
    sys.exit(1)


class MedGemmaLoRATrainer:
    """Handles LoRA fine-tuning for MedGemma with vision features."""
    
    def __init__(
        self,
        base_model: str = "google/medgemma-2b",
        lora_rank: int = 16,
        lora_alpha: int = 32,
        lora_dropout: float = 0.1,
        vision_dim: int = 512,
        device: str = "auto"
    ):
        self.base_model_name = base_model
        self.lora_rank = lora_rank
        self.lora_alpha = lora_alpha
        self.lora_dropout = lora_dropout
        self.vision_dim = vision_dim
        
        # Setup device
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
        else:
            self.device = device
        
        print(f"üñ•Ô∏è  Using device: {self.device}")
        
        # Load tokenizer
        print(f"üì¶ Loading tokenizer from {base_model}...")
        self.tokenizer = AutoTokenizer.from_pretrained(base_model)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Load base model
        print(f"üß† Loading base model {base_model}...")
        self.model = None  # Lazy load
        
    def load_model(self, use_8bit: bool = False):
        """Load and prepare model with LoRA."""
        if self.model is not None:
            return
        
        # Load with optional quantization
        load_kwargs = {
            'device_map': 'auto' if self.device == "cuda" else None,
            'torch_dtype': torch.float16 if self.device == "cuda" else torch.float32
        }
        
        if use_8bit and self.device == "cuda":
            load_kwargs['load_in_8bit'] = True
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            **load_kwargs
        )
        
        # Prepare for LoRA
        if use_8bit:
            self.model = prepare_model_for_kbit_training(self.model)
        
        # Configure LoRA
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=self.lora_rank,
            lora_alpha=self.lora_alpha,
            lora_dropout=self.lora_dropout,
            target_modules=["q_proj", "v_proj", "o_proj"],  # Attention layers
            bias="none"
        )
        
        # Apply LoRA
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
    
    def load_dataset(self, train_path: Path, val_path: Path) -> tuple:
        """Load and preprocess training data."""
        print(f"\nüìÇ Loading dataset...")
        print(f"  Train: {train_path}")
        print(f"  Val: {val_path}")
        
        # Load JSONL
        train_examples = []
        with open(train_path) as f:
            for line in f:
                train_examples.append(json.loads(line))
        
        val_examples = []
        with open(val_path) as f:
            for line in f:
                val_examples.append(json.loads(line))
        
        print(f"  Loaded {len(train_examples)} train, {len(val_examples)} val examples")
        
        # Convert to Hugging Face Dataset
        train_dataset = Dataset.from_list(train_examples)
        val_dataset = Dataset.from_list(val_examples)
        
        # Tokenize
        def preprocess(examples):
            # Combine instruction + input + output
            texts = []
            for i in range(len(examples['instruction'])):
                text = f"{examples['instruction'][i]}\n\n{examples['input'][i]}\n{examples['output'][i]}"
                texts.append(text)
            
            # Tokenize
            tokenized = self.tokenizer(
                texts,
                padding='max_length',
                truncation=True,
                max_length=512,
                return_tensors='pt'
            )
            
            # Labels = input_ids for causal LM
            tokenized['labels'] = tokenized['input_ids'].clone()
            
            return tokenized
        
        train_dataset = train_dataset.map(
            preprocess,
            batched=True,
            remove_columns=train_dataset.column_names
        )
        
        val_dataset = val_dataset.map(
            preprocess,
            batched=True,
            remove_columns=val_dataset.column_names
        )
        
        return train_dataset, val_dataset
    
    def train(
        self,
        train_dataset: Dataset,
        val_dataset: Dataset,
        output_dir: Path,
        num_epochs: int = 3,
        batch_size: int = 4,
        learning_rate: float = 2e-4,
        gradient_accumulation_steps: int = 4
    ):
        """Run LoRA fine-tuning."""
        print(f"\nüèãÔ∏è  Starting training...")
        print(f"  Epochs: {num_epochs}")
        print(f"  Batch Size: {batch_size}")
        print(f"  Learning Rate: {learning_rate}")
        print(f"  Gradient Accumulation: {gradient_accumulation_steps}")
        
        # Training arguments
        training_args = TrainingArguments(
            output_dir=str(output_dir),
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            learning_rate=learning_rate,
            weight_decay=0.01,
            warmup_steps=100,
            logging_steps=10,
            evaluation_strategy="epoch",
            save_strategy="epoch",
            save_total_limit=2,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            fp16=self.device == "cuda",
            report_to="none"
        )
        
        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False  # Causal LM, not masked LM
        )
        
        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            data_collator=data_collator
        )
        
        # Train!
        print("\nüöÄ Training started...")
        train_result = trainer.train()
        
        # Save
        print(f"\nüíæ Saving model to {output_dir}...")
        trainer.save_model()
        
        # Metrics
        metrics = train_result.metrics
        print("\n" + "=" * 80)
        print("Training Complete!")
        print("=" * 80)
        print(f"Train Loss: {metrics['train_loss']:.4f}")
        print(f"Train Runtime: {metrics['train_runtime']:.2f}s")
        print(f"Samples/Second: {metrics['train_samples_per_second']:.2f}")
        
        return trainer, metrics


def main():
    parser = argparse.ArgumentParser(description="Fine-tune MedGemma with LoRA")
    parser.add_argument('--model', default='google/medgemma-2b', help='Base model name')
    parser.add_argument('--data-dir', default='data/lora_training', help='Dataset directory')
    parser.add_argument('--output-dir', default='models/medgemma-lora', help='Output directory')
    parser.add_argument('--epochs', type=int, default=3, help='Training epochs')
    parser.add_argument('--batch-size', type=int, default=4, help='Batch size')
    parser.add_argument('--lr', type=float, default=2e-4, help='Learning rate')
    parser.add_argument('--lora-rank', type=int, default=16, help='LoRA rank')
    parser.add_argument('--lora-alpha', type=int, default=32, help='LoRA alpha')
    parser.add_argument('--use-8bit', action='store_true', help='Use 8-bit quantization')
    parser.add_argument('--dry-run', action='store_true', help='Verify setup without training')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("MedGemma LoRA Fine-Tuning - Phase 2")
    print("=" * 80)
    
    # Setup paths
    data_dir = PROJECT_ROOT / args.data_dir
    output_dir = PROJECT_ROOT / args.output_dir
    output_dir.mkdir(exist_ok=True, parents=True)
    
    train_path = data_dir / 'train.jsonl'
    val_path = data_dir / 'val.jsonl'
    
    if not train_path.exists():
        print(f"‚ùå Training data not found: {train_path}")
        print("\nRun first:")
        print("  python3 scripts/prepare_lora_dataset.py")
        return 1
    
    # Initialize trainer
    trainer = MedGemmaLoRATrainer(
        base_model=args.model,
        lora_rank=args.lora_rank,
        lora_alpha=args.lora_alpha
    )
    
    # Load dataset
    train_dataset, val_dataset = trainer.load_dataset(train_path, val_path)
    
    if args.dry_run:
        print("\n‚úÖ Dry run complete - setup verified!")
        print("\nTo train:")
        print("  python3 scripts/finetune_medgemma_lora.py")
        return 0
    
    # Load model with LoRA
    trainer.load_model(use_8bit=args.use_8bit)
    
    # Train
    trained_model, metrics = trainer.train(
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        output_dir=output_dir,
        num_epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.lr
    )
    
    # Save training metadata
    metadata = {
        'base_model': args.model,
        'lora_rank': args.lora_rank,
        'lora_alpha': args.lora_alpha,
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'learning_rate': args.lr,
        'train_samples': len(train_dataset),
        'val_samples': len(val_dataset),
        'final_train_loss': metrics['train_loss'],
        'trained_at': datetime.now().isoformat()
    }
    
    with open(output_dir / 'training_metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print("\n" + "=" * 80)
    print("‚úÖ Fine-tuning complete!")
    print("=" * 80)
    print(f"\nModel saved to: {output_dir}")
    print(f"\nNext steps:")
    print(f"  1. Test: python3 scripts/test_finetuned_medgemma.py")
    print(f"  2. Benchmark: python3 scripts/run_clinical_validation_benchmarks.py --model medgemma-lora")
    print(f"  3. Compare performance against baseline")
    
    return 0


if __name__ == '__main__':
    sys.exit(main())
